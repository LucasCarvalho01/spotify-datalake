{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e287060-d14c-4547-91f9-a80233e9ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"spotify-datalake\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1024M\") \\\n",
    "    .config(\"spark.ui.port\", \"4061\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c401f6c-9b46-406d-a3ff-a08f90803c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[album_name: string, album_uri: string, artist_name: string, artist_uri: string, duration_ms: bigint, pid: bigint, pos: bigint, track_name: string, track_uri: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists_v1_path = '/shared/sampled/playlists_v1.json'\n",
    "playlists_v1_df = spark.read.json(playlists_v1_path)\n",
    "tracks_v1_path = '/shared/sampled/tracks_v1.json'\n",
    "tracks_v1_df = spark.read.json(tracks_v1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07907de-b8b5-449f-8835-d58baf0a9060",
   "metadata": {},
   "source": [
    "Task 1A:\n",
    "- Silver layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c017094e-9c6e-4957-908d-88604282d9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 15:16:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Buttons', track_uri='spotify:track:3BxWKCI06eQ5Od8TY2JBeA', duration_ms=225560, album_uri='spotify:album:5x8e8UcCeOgrOzSnDGuPye', artist_uri='spotify:artist:6wPhSqRtPu1UhRCDX5yaDJ'),\n",
       " Row(name='Say My Name', track_uri='spotify:track:7H6ev70Weq6DdpZyyTmUXk', duration_ms=271333, album_uri='spotify:album:283NWqNsCA9GwVHrJk59CG', artist_uri='spotify:artist:1Y8cdNmUJH7yBTd9yOvr5i'),\n",
       " Row(name='Hey Ya! - Radio Mix / Club Mix', track_uri='spotify:track:2PpruBYCo4H7WOBJ7Q2EwM', duration_ms=235213, album_uri='spotify:album:1UsmQ3bpJTyK6ygoOOjG1r', artist_uri='spotify:artist:1G9G7WwrXka3Z1r7aIDjI7'),\n",
       " Row(name='Promiscuous', track_uri='spotify:track:2gam98EZKrF9XuOkU13ApN', duration_ms=242293, album_uri='spotify:album:2yboV2QBcVGEhcRlYuPpDT', artist_uri='spotify:artist:2jw70GZXlAI8QzWeY2bgRc'),\n",
       " Row(name='Right Where You Want Me - Radio Edit Version', track_uri='spotify:track:4Y45aqo9QMa57rDsAJv40A', duration_ms=211693, album_uri='spotify:album:6022khQj4Fsvvse8f3A4lF', artist_uri='spotify:artist:2Hjj68yyUPiC0HKEOigcEp')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "song_info_df = tracks_v1_df.select(\n",
    "    F.col(\"track_name\").alias(\"name\"),\n",
    "    F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "    F.col(\"duration_ms\").alias(\"duration_ms\"),\n",
    "    F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    ")\n",
    "\n",
    "song_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/song_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6535479d-4a44-4209-8f84-1b9e3aa9d5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 15:18:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "album_info_df = tracks_v1_df.select(\n",
    "    F.col(\"album_name\").alias(\"name\"),\n",
    "    F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    ").distinct()\n",
    "\n",
    "album_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/album_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8edad837-3dc3-423e-a083-7820b2b2f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_info_df = tracks_v1_df.select(\n",
    "    F.col(\"artist_name\").alias(\"name\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    ").distinct()\n",
    "\n",
    "artist_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/artist_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e15c08a-eeb8-4cc3-82a0-a5d3cbcec746",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 4be036a0-5ddc-492e-8c36-0e3f68b77eee).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- name: string (nullable = true)\n-- playlist_id: long (nullable = true)\n-- description: string (nullable = true)\n-- is_collaborative: string (nullable = true)\n\n\nData schema:\nroot\n-- name: string (nullable = true)\n-- pid: long (nullable = true)\n-- description: string (nullable = true)\n-- is_collaborative: string (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m playlist_info_df \u001b[38;5;241m=\u001b[39m playlists_v1_df\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m      2\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollaborative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_collaborative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mplaylist_info_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatalake/silver/playlist_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py:1398\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 4be036a0-5ddc-492e-8c36-0e3f68b77eee).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- name: string (nullable = true)\n-- playlist_id: long (nullable = true)\n-- description: string (nullable = true)\n-- is_collaborative: string (nullable = true)\n\n\nData schema:\nroot\n-- name: string (nullable = true)\n-- pid: long (nullable = true)\n-- description: string (nullable = true)\n-- is_collaborative: string (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
     ]
    }
   ],
   "source": [
    "playlist_info_df = playlists_v1_df.select(\n",
    "    F.col(\"name\").alias(\"name\"),\n",
    "    F.col(\"pid\").alias(\"pid\"),\n",
    "    F.col(\"description\").alias(\"description\"),\n",
    "    F.col(\"collaborative\").alias(\"is_collaborative\")\n",
    ")\n",
    "\n",
    "playlist_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/playlist_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "982bc341-efc6-4640-8880-0a7ba6425521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 15:22:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlist_tracks_df = tracks_v1_df.select(\n",
    "    F.col(\"pid\").alias(\"playlist_id\"),\n",
    "    F.col(\"pos\").alias(\"position\"),\n",
    "    F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\"),\n",
    "    F.col(\"album_uri\").alias(\"album_uri\")\n",
    ")\n",
    "\n",
    "playlist_tracks_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/playlist_tracks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe088a-1a82-4547-a2b3-9af8952a1af8",
   "metadata": {},
   "source": [
    "- Gold layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c65fae39-59cc-42f0-b99b-b617a5f60678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlist_info_gold_df = tracks_v1_df.groupBy(\"pid\").agg(\n",
    "    F.count(\"track_uri\").alias(\"num_tracks\"),\n",
    "    F.sum(\"duration_ms\").alias(\"total_duration_ms\"),\n",
    "    F.countDistinct(\"artist_uri\").alias(\"num_artists\"),\n",
    "    F.countDistinct(\"album_uri\").alias(\"num_albums\")\n",
    ").join(playlist_info_df, \"pid\", \"inner\")\n",
    "\n",
    "playlist_info_gold_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/gold/playlist_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c3371fa-a23d-4a7b-8730-0662578f5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_info_df = song_info_df.withColumnRenamed(\"name\", \"song_name\")\n",
    "album_info_df = album_info_df.withColumnRenamed(\"name\", \"album_name\")\n",
    "artist_info_df = artist_info_df.withColumnRenamed(\"name\", \"artist_name\")\n",
    "\n",
    "playlist_tracks_gold_df = playlist_tracks_df.join(\n",
    "    song_info_df, \"track_uri\", \"inner\"\n",
    ").join(\n",
    "    album_info_df, \"album_uri\", \"inner\"\n",
    ").join(\n",
    "    artist_info_df, \"artist_uri\", \"inner\"\n",
    ").select(\n",
    "    \"playlist_id\",\n",
    "    \"position\",\n",
    "    \"song_name\",\n",
    "    \"album_name\",\n",
    "    \"artist_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b443932-f753-4c7a-9dbb-e400a2de8deb",
   "metadata": {},
   "source": [
    "Task 1B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adde54b5-bb87-4b13-aeac-b59de47ed58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de carregamento (JSON): 0.3263206481933594 segundos\n"
     ]
    }
   ],
   "source": [
    "playlist_info_gold_df.write.format(\"json\").mode(\"overwrite\").save(\"datalake/gold_json/playlist_info\")\n",
    "playlist_tracks_gold_df.write.format(\"json\").mode(\"overwrite\").save(\"datalake/gold_json/playlist_tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "523c46ba-ae78-4a70-8b4c-c87db16749d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 16:28:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/01 16:28:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de carregamento (Parquet): 0.6755235195159912 segundos\n"
     ]
    }
   ],
   "source": [
    "playlist_info_gold_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/gold_parquet/playlist_info\")\n",
    "playlist_tracks_gold_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/gold_parquet/playlist_tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0dd4f6e-e0b0-46d8-9606-52f12646b61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de carregamento (JSON): 0.3448805809020996 segundos\n",
      "Tempo de carregamento (Parquet): 0.2278423309326172 segundos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# tempo de load em json\n",
    "start_time = time.time()\n",
    "json_df = spark.read.json(\"datalake/gold_json/playlist_info\")\n",
    "json_df.count()\n",
    "json_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Tempo de carregamento (JSON): {json_load_time} segundos\")\n",
    "\n",
    "# tempo de load em parquet\n",
    "start_time = time.time()\n",
    "parquet_df = spark.read.parquet(\"datalake/gold_parquet/playlist_info\")\n",
    "parquet_df.count()\n",
    "parquet_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Tempo de carregamento (Parquet): {parquet_load_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5005e",
   "metadata": {},
   "source": [
    "**Task 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5722d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = \"datalake/bronze/\"\n",
    "silver_path = \"datalake/silver/\"\n",
    "gold_path = \"datalake/gold_parquet/\"\n",
    "\n",
    "def ingest_new_data(version):\n",
    "    playlists = spark.read.json(f\"/shared/sampled/playlists_{version}.json\")\n",
    "    tracks = spark.read.json(f\"/shared/sampled/tracks_{version}.json\")\n",
    "\n",
    "    playlists.write.mode(\"append\").parquet(bronze_path + \"playlists/\")\n",
    "    tracks.write.mode(\"append\").parquet(bronze_path + \"tracks/\")\n",
    "\n",
    "    return playlists, tracks\n",
    "\n",
    "def update_silver_layer(playlists_v, tracks_v):\n",
    "    \n",
    "    song_info = spark.read.parquet(silver_path + \"song_info\")\n",
    "    album_info = spark.read.parquet(silver_path + \"album_info\")\n",
    "    artist_info = spark.read.parquet(silver_path + \"artist_info\")\n",
    "    playlist_info = spark.read.parquet(silver_path + \"playlist_info\")\n",
    "    playlist_tracks = spark.read.parquet(silver_path + \"playlist_tracks\")\n",
    "\n",
    "    new_song_info = tracks_v.select(\n",
    "        F.col(\"track_name\").alias(\"name\"),\n",
    "        F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "        F.col(\"duration_ms\").alias(\"duration_ms\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "    song_info = song_info.unionByName(new_song_info, allowMissingColumns=True).dropDuplicates([\"track_uri\"])\n",
    "\n",
    "    new_album_info = tracks_v.select(\n",
    "        F.col(\"album_name\").alias(\"name\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "    album_info = album_info.unionByName(new_album_info, allowMissingColumns=True).dropDuplicates([\"album_uri\"])\n",
    "\n",
    "    new_artist_info = tracks_v.select(\n",
    "        F.col(\"artist_name\").alias(\"name\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "    artist_info = artist_info.unionByName(new_artist_info, allowMissingColumns=True).dropDuplicates([\"artist_uri\"])\n",
    "\n",
    "    new_playlist_info = playlists_v.select(\n",
    "        F.col(\"name\").alias(\"name\"),\n",
    "        F.col(\"pid\").alias(\"pid\"),\n",
    "        F.col(\"description\").alias(\"description\"),\n",
    "        F.col(\"collaborative\").alias(\"is_collaborative\")\n",
    "    ).distinct()\n",
    "    playlist_info = playlist_info.unionByName(new_playlist_info, allowMissingColumns=True).dropDuplicates([\"playlist_id\"])\n",
    "\n",
    "    new_playlist_tracks = tracks_v.select(\n",
    "        F.col(\"pid\").alias(\"playlist_id\"),\n",
    "        F.col(\"pos\").alias(\"position\"),\n",
    "        F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\")\n",
    "    )\n",
    "    playlist_tracks = playlist_tracks.unionByName(new_playlist_tracks, allowMissingColumns=True).dropDuplicates([\"playlist_id\", \"track_uri\"])\n",
    "\n",
    "    playlist_info = playlist_info.withColumn(\n",
    "        \"name\", F.when(F.col(\"playlist_id\") == \"11992\", \"GYM WORKOUT\").otherwise(F.col(\"name\"))\n",
    "    ).withColumn(\n",
    "        \"collaborative\", F.when(F.col(\"playlist_id\") == \"11992\", True).otherwise(F.col(\"collaborative\"))\n",
    "    )\n",
    "\n",
    "    song_info.write.mode(\"overwrite\").parquet(silver_path + \"song_info\")\n",
    "    album_info.write.mode(\"overwrite\").parquet(silver_path + \"album_info\")\n",
    "    artist_info.write.mode(\"overwrite\").parquet(silver_path + \"artist_info\")\n",
    "    playlist_info.write.mode(\"overwrite\").parquet(silver_path + \"playlist_info\")\n",
    "    playlist_tracks.write.mode(\"overwrite\").parquet(silver_path + \"playlist_tracks\")\n",
    "\n",
    "def update_gold_layer():\n",
    "    playlist_info = spark.read.parquet(silver_path + \"playlist_info\")\n",
    "    playlist_tracks = spark.read.parquet(silver_path + \"playlist_tracks\")\n",
    "\n",
    "    playlists_gold = playlist_info.join(playlist_tracks, \"playlist_id\", \"left\") \\\n",
    "        .groupBy(\"playlist_id\", \"name\", \"description\") \\\n",
    "        .agg(\n",
    "            F.count(\"track_uri\").alias(\"num_tracks\"),\n",
    "            F.sum(\"duration_ms\").alias(\"total_duration_ms\"),\n",
    "            F.countDistinct(\"artist_uri\").alias(\"num_artists\"),\n",
    "            F.countDistinct(\"album_uri\").alias(\"num_albums\")\n",
    "        )\n",
    "\n",
    "    playlist_tracks_gold = playlist_tracks.join(\n",
    "        song_info_df, \"track_uri\", \"inner\"\n",
    "    ).join(\n",
    "        album_info_df, \"album_uri\", \"inner\"\n",
    "    ).join(\n",
    "        artist_info_df, \"artist_uri\", \"inner\"\n",
    "    ).select(\n",
    "        \"playlist_id\",\n",
    "        \"position\",\n",
    "        \"song_name\",\n",
    "        \"album_name\",\n",
    "        \"artist_name\"\n",
    "    )\n",
    "\n",
    "    playlists_gold.write.mode(\"overwrite\").parquet(gold_path + \"playlist_info\")\n",
    "    playlist_tracks_gold.write.mode(\"overwrite\").parquet(gold_path + \"playlist_tracks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(version_path):\n",
    "    print(f\"Iniciando ingestão dos dados de {version_path}...\")\n",
    "    playlists, tracks = ingest_new_data(version_path)\n",
    "\n",
    "    print(\"Atualizando a camada Silver...\")\n",
    "    update_silver_layer(playlists, tracks)\n",
    "\n",
    "    print(\"Atualizando a camada Gold...\")\n",
    "    update_gold_layer()\n",
    "\n",
    "    print(f\"Pipeline concluído para {version_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pipeline(\"v2\")\n",
    "run_pipeline(\"v3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
