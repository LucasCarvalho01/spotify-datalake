{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85cbc764",
   "metadata": {},
   "source": [
    "Integrantes:\n",
    "Lucas Pereira Carvalho\n",
    "Lucas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e287060-d14c-4547-91f9-a80233e9ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"spotify-datalake\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1024M\") \\\n",
    "    .config(\"spark.ui.port\", \"4061\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c401f6c-9b46-406d-a3ff-a08f90803c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlists_v1_path = '/shared/sampled/playlists_v1.json'\n",
    "playlists_v1_df = spark.read.json(playlists_v1_path)\n",
    "tracks_v1_path = '/shared/sampled/tracks_v1.json'\n",
    "tracks_v1_df = spark.read.json(tracks_v1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07907de-b8b5-449f-8835-d58baf0a9060",
   "metadata": {},
   "source": [
    "# Task 1A:\n",
    "## Silver layer:\n",
    "  \n",
    "Foram criadas as seguintes tabelas para a camada silver e seus respectivos esquemas:\n",
    "\n",
    "- song_info\n",
    "    name (string): Nome da música\n",
    "    track_uri (string): URI da música\n",
    "    duration_ms (long): Duração da música em ms\n",
    "    album_uri (string): URI do album\n",
    "    artist_uri (string): URI do artista\n",
    "\n",
    "- album_info\n",
    "    name (string): Nome do album\n",
    "    album_uri (string): URI do album\n",
    "    artist_uri (string): URI do artista\n",
    "\n",
    "- artist_info\n",
    "    name (string): Nome do artista\n",
    "    artist_uri (string): URI do artista\n",
    "\n",
    "- playlist_info\n",
    "    name (string): Nome da playlist\n",
    "    playlist_id (int): ID da playlist\n",
    "    description (string): Descrição da playlist\n",
    "    is_collaborative (boolean): Indica se a playlist é colaborativa\n",
    "\n",
    "- playlist_tracks\n",
    "    playlist_id (int): ID da playlist\n",
    "    position (int): Posição da música na playlist\n",
    "    track_uri (string): URI da música\n",
    "    artist_uri (string): URI do artista\n",
    "    album_uri (string): URI do álbum\n",
    "\n",
    "Nesta etapa, foram realizadas as seguintes modificações:\n",
    "\n",
    "- Seleção de colunas relevantes:\n",
    "  Extraída apenas as colunas necessárias para cada tabela na camada Silver, removendo colunas desnecessárias\n",
    "- Remoção de duplicatas:\n",
    "  Aplicado distinct() nas tabelas para evitar duplicatas\n",
    "- Renomeação de colunas:\n",
    "  Renomeada colunas para nomes mais descritivos, como track_name para name\n",
    "- Conversão de tipos de dados:\n",
    "  Convertido colunas como collaborative para boolean, facilitando a computação em cima dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c017094e-9c6e-4957-908d-88604282d9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 22:51:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "song_info_df = tracks_v1_df.select(\n",
    "    F.col(\"track_name\").alias(\"name\"),\n",
    "    F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "    F.col(\"duration_ms\").alias(\"duration_ms\"),\n",
    "    F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    ")\n",
    "\n",
    "song_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/song_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6535479d-4a44-4209-8f84-1b9e3aa9d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "album_info_df = tracks_v1_df.select(\n",
    "    F.col(\"album_name\").alias(\"name\"),\n",
    "    F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    ").distinct()\n",
    "\n",
    "album_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/album_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8edad837-3dc3-423e-a083-7820b2b2f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_info_df = tracks_v1_df.select(\n",
    "    F.col(\"artist_name\").alias(\"name\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    ").distinct()\n",
    "\n",
    "artist_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/artist_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e15c08a-eeb8-4cc3-82a0-a5d3cbcec746",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_info_df = playlists_v1_df.select(\n",
    "    F.col(\"name\").alias(\"name\"),\n",
    "    F.col(\"pid\").alias(\"pid\"),\n",
    "    F.col(\"description\").alias(\"description\"),\n",
    "    F.col(\"collaborative\").alias(\"is_collaborative\")\n",
    ")\n",
    "\n",
    "playlist_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/playlist_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "982bc341-efc6-4640-8880-0a7ba6425521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 22:51:29 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "playlist_tracks_df = tracks_v1_df.select(\n",
    "    F.col(\"pid\").alias(\"playlist_id\"),\n",
    "    F.col(\"pos\").alias(\"position\"),\n",
    "    F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\"),\n",
    "    F.col(\"album_uri\").alias(\"album_uri\")\n",
    ")\n",
    "\n",
    "playlist_tracks_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/playlist_tracks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe088a-1a82-4547-a2b3-9af8952a1af8",
   "metadata": {},
   "source": [
    "## Gold layer:\n",
    "\n",
    "Foram criadas as seguintes tabelas para a camada gold e seus respectivos esquemas:\n",
    "\n",
    "- playlist_info_gold:\n",
    "    name (string): Nome da playlist\n",
    "    playlist_id (int): ID da playlist\n",
    "    description (string): Descrição da playlist\n",
    "    total_duration_ms (long): Duração total da playlist em ms\n",
    "    num_tracks (int): Numero de msuicas na playlist\n",
    "    num_artists (int): Numero de artistas únicos na playlist\n",
    "    num_albums (int): Numero de álbuns únicos na playlist\n",
    "\n",
    "- playlist_tracks_gold\n",
    "    playlist_id (int): ID da playlist\n",
    "    position (int): Posição da música na playlist\n",
    "    song_name (string): Nome da música\n",
    "    album_name (string): Nome do álbum\n",
    "    artist_name (string): Nome do artista\n",
    "\n",
    "Nesta etapa foram feitas as seguintes transformações nos dados:\n",
    "- Calculada métricas total_duration_ms, num_tracks, num_artists, e num_albums de forma agregada para a tabela playlist_info_gold\n",
    "- Combinada as tabelas playlist_tracks, song_info, album_info, e artist_info para criar playlist_tracks_gold\n",
    "- Ordenado as músicas em playlist_tracks_gold pela coluna position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c65fae39-59cc-42f0-b99b-b617a5f60678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlist_info_gold_df = tracks_v1_df.groupBy(\"pid\").agg(\n",
    "    F.count(\"track_uri\").alias(\"num_tracks\"),\n",
    "    F.sum(\"duration_ms\").alias(\"total_duration_ms\"),\n",
    "    F.countDistinct(\"artist_uri\").alias(\"num_artists\"),\n",
    "    F.countDistinct(\"album_uri\").alias(\"num_albums\")\n",
    ").join(playlist_info_df, \"pid\", \"inner\")\n",
    "\n",
    "playlist_info_gold_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/gold/playlist_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c3371fa-a23d-4a7b-8730-0662578f5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_info_df = song_info_df.withColumnRenamed(\"name\", \"song_name\")\n",
    "album_info_df = album_info_df.withColumnRenamed(\"name\", \"album_name\")\n",
    "artist_info_df = artist_info_df.withColumnRenamed(\"name\", \"artist_name\")\n",
    "\n",
    "playlist_tracks_gold_df = playlist_tracks_df.join(\n",
    "    song_info_df, \"track_uri\", \"inner\"\n",
    ").join(\n",
    "    album_info_df, \"album_uri\", \"inner\"\n",
    ").join(\n",
    "    artist_info_df, \"artist_uri\", \"inner\"\n",
    ").select(\n",
    "    \"playlist_id\",\n",
    "    \"position\",\n",
    "    \"song_name\",\n",
    "    \"album_name\",\n",
    "    \"artist_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b443932-f753-4c7a-9dbb-e400a2de8deb",
   "metadata": {},
   "source": [
    "# Task 1B:\n",
    "\n",
    "Nesta tarefa, foram salvas as tabelas playlist_info_gold_df e playlist_tracks_gold_df em ambos os formatos, json e parquet.\n",
    "Após isso, com auxílio da lib time, contamos o tempo levado para fazer a leitura do dataframe, para ambos os formatos. Pôde-se verificar um melhor desempenho no formato parquet, conforme esperado, já que se trata de um formato mais otimizado para armazenamento e leitura dos dados, em comparação ao json.\n",
    "Código usado para possível visualização dessa diferença de tempo a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "adde54b5-bb87-4b13-aeac-b59de47ed58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlist_info_gold_df.write.format(\"json\").mode(\"overwrite\").save(\"datalake/gold_json/playlist_info\")\n",
    "playlist_tracks_gold_df.write.format(\"json\").mode(\"overwrite\").save(\"datalake/gold_json/playlist_tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "523c46ba-ae78-4a70-8b4c-c87db16749d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 22:51:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 22:52:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlist_info_gold_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/gold_parquet/playlist_info\")\n",
    "playlist_tracks_gold_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/gold_parquet/playlist_tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0dd4f6e-e0b0-46d8-9606-52f12646b61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de carregamento (JSON): 0.25846004486083984 segundos\n",
      "Tempo de carregamento (Parquet): 0.16401124000549316 segundos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# tempo de load em json\n",
    "start_time = time.time()\n",
    "json_df = spark.read.json(\"datalake/gold_json/playlist_info\")\n",
    "json_df.count()\n",
    "json_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Tempo de carregamento (JSON): {json_load_time} segundos\")\n",
    "\n",
    "# tempo de load em parquet\n",
    "start_time = time.time()\n",
    "parquet_df = spark.read.parquet(\"datalake/gold_parquet/playlist_info\")\n",
    "parquet_df.count()\n",
    "parquet_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Tempo de carregamento (Parquet): {parquet_load_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5005e",
   "metadata": {},
   "source": [
    "**Task 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f5722d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = \"datalake/bronze/\"\n",
    "silver_path = \"datalake/silver/\"\n",
    "gold_path = \"datalake/gold_parquet/\"\n",
    "\n",
    "def get_song_info(tracks_v):\n",
    "    return tracks_v.select(\n",
    "        F.col(\"track_name\").alias(\"name\"),\n",
    "        F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "        F.col(\"duration_ms\").alias(\"duration_ms\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "\n",
    "def get_album_info(tracks_v):\n",
    "    return tracks_v.select(\n",
    "        F.col(\"album_name\").alias(\"name\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "\n",
    "def get_artist_info(tracks_v):\n",
    "    return tracks_v.select(\n",
    "        F.col(\"artist_name\").alias(\"name\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "\n",
    "def get_playlist_info(playlists_v):\n",
    "    return playlists_v.select(\n",
    "        F.col(\"name\").alias(\"name\"),\n",
    "        F.col(\"pid\").alias(\"pid\"),\n",
    "        F.col(\"description\").alias(\"description\"),\n",
    "        F.col(\"collaborative\").alias(\"is_collaborative\")\n",
    "    ).distinct()\n",
    "\n",
    "def get_playlist_track_info(tracks_v):\n",
    "    return tracks_v.select(\n",
    "        F.col(\"pid\").alias(\"playlist_id\"),\n",
    "        F.col(\"pos\").alias(\"position\"),\n",
    "        F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\")\n",
    "    )\n",
    "\n",
    "def ingest_new_data(version):\n",
    "    playlists = spark.read.json(f\"/shared/sampled/playlists_{version}.json\")\n",
    "    tracks = spark.read.json(f\"/shared/sampled/tracks_{version}.json\")\n",
    "\n",
    "    playlists.write.mode(\"append\").parquet(bronze_path + \"playlists/\")\n",
    "    tracks.write.mode(\"append\").parquet(bronze_path + \"tracks/\")\n",
    "\n",
    "    return playlists, tracks\n",
    "\n",
    "def update_silver_layer(playlists_v, tracks_v):\n",
    "    \n",
    "    song_info = spark.read.parquet(silver_path + \"song_info\")\n",
    "    album_info = spark.read.parquet(silver_path + \"album_info\")\n",
    "    artist_info = spark.read.parquet(silver_path + \"artist_info\")\n",
    "    playlist_info = spark.read.parquet(silver_path + \"playlist_info\")\n",
    "    playlist_tracks = spark.read.parquet(silver_path + \"playlist_tracks\")\n",
    "\n",
    "    new_song_info = get_song_info(tracks_v)\n",
    "    song_info = song_info.unionByName(new_song_info, allowMissingColumns=True).dropDuplicates([\"track_uri\"])\n",
    "\n",
    "    new_album_info = get_album_info(tracks_v)\n",
    "    album_info = album_info.unionByName(new_album_info, allowMissingColumns=True).dropDuplicates([\"album_uri\"])\n",
    "\n",
    "    new_artist_info = get_artist_info(tracks_v)\n",
    "    artist_info = artist_info.unionByName(new_artist_info, allowMissingColumns=True).dropDuplicates([\"artist_uri\"])\n",
    "\n",
    "    new_playlist_info = get_playlist_info(playlists_v)\n",
    "    playlist_info = playlist_info.unionByName(new_playlist_info, allowMissingColumns=True).dropDuplicates([\"pid\"])\n",
    "\n",
    "    new_playlist_tracks = get_playlist_track_info(tracks_v)\n",
    "    playlist_tracks = playlist_tracks.unionByName(new_playlist_tracks, allowMissingColumns=True).dropDuplicates([\"playlist_id\", \"track_uri\"])\n",
    "\n",
    "    playlist_info = playlist_info.withColumn(\n",
    "        \"name\", F.when(F.col(\"pid\") == \"11992\", \"GYM WORKOUT\").otherwise(F.col(\"name\"))\n",
    "    ).withColumn(\n",
    "        \"is_collaborative\", F.when(F.col(\"pid\") == \"11992\", F.lit(True).cast(\"string\")).otherwise(F.col(\"is_collaborative\"))\n",
    "    )\n",
    "\n",
    "    song_info.write.mode(\"overwrite\").parquet(silver_path + \"song_info\")\n",
    "    album_info.write.mode(\"overwrite\").parquet(silver_path + \"album_info\")\n",
    "    artist_info.write.mode(\"overwrite\").parquet(silver_path + \"artist_info\")\n",
    "    playlist_info.write.mode(\"overwrite\").parquet(silver_path + \"playlist_info\")\n",
    "    playlist_tracks.write.mode(\"overwrite\").parquet(silver_path + \"playlist_tracks\")\n",
    "\n",
    "def update_gold_layer():\n",
    "    playlist_info = spark.read.parquet(silver_path + \"playlist_info\")\n",
    "    song_info = spark.read.parquet(silver_path + \"song_info\")\n",
    "    album_info = spark.read.parquet(silver_path + \"album_info\")\n",
    "    artist_info = spark.read.parquet(silver_path + \"artist_info\")\n",
    "    playlist_tracks = spark.read.parquet(silver_path + \"playlist_tracks\")\n",
    "\n",
    "    song_info = song_info_df.withColumnRenamed(\"name\", \"song_name\")\n",
    "    album_info = album_info_df.withColumnRenamed(\"name\", \"album_name\")\n",
    "    artist_info = artist_info_df.withColumnRenamed(\"name\", \"artist_name\")\n",
    "\n",
    "    playlists_gold = playlist_info.alias(\"pi\") \\\n",
    "        .join(playlist_tracks.alias(\"pt\"), F.col(\"pi.pid\") == F.col(\"pt.playlist_id\"), \"left\") \\\n",
    "        .join(song_info.alias(\"si\"), F.col(\"pt.track_uri\") == F.col(\"si.track_uri\"), \"left\") \\\n",
    "        .groupBy(\"pi.pid\", \"pi.name\", \"pi.description\") \\\n",
    "        .agg(\n",
    "            F.count(\"pt.track_uri\").alias(\"num_tracks\"),\n",
    "            F.sum(\"si.duration_ms\").alias(\"total_duration_ms\"),\n",
    "            F.countDistinct(\"pt.artist_uri\").alias(\"num_artists\"),\n",
    "            F.countDistinct(\"pt.album_uri\").alias(\"num_albums\")\n",
    "    )\n",
    "\n",
    "    playlist_tracks_gold = playlist_tracks.join(\n",
    "            song_info.alias(\"si\"), \"track_uri\", \"inner\"\n",
    "        ).join(\n",
    "            album_info.alias(\"ali\"), \"album_uri\", \"inner\"\n",
    "        ).join(\n",
    "            artist_info.alias(\"ari\"), \"artist_uri\", \"inner\"\n",
    "        ).select(\n",
    "            \"playlist_id\",\n",
    "            \"position\",\n",
    "            \"song_name\",\n",
    "            \"album_name\",\n",
    "            \"artist_name\"\n",
    "    )\n",
    "\n",
    "    playlists_gold.write.mode(\"overwrite\").parquet(gold_path + \"playlist_info\")\n",
    "    playlist_tracks_gold.write.mode(\"overwrite\").parquet(gold_path + \"playlist_tracks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6c31423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(version_path):\n",
    "    print(f\"Iniciando ingestão dos dados de {version_path}...\")\n",
    "    playlists, tracks = ingest_new_data(version_path)\n",
    "\n",
    "    print(\"Atualizando a camada Silver...\")\n",
    "    update_silver_layer(playlists, tracks)\n",
    "\n",
    "    print(\"Atualizando a camada Gold...\")\n",
    "    update_gold_layer()\n",
    "\n",
    "    print(f\"Pipeline concluído para {version_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "13fd205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ingestão dos dados de v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 22:57:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Silver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 22:57:30 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 22:57:31 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 22:57:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Gold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 22:57:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:57:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 22:58:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 788:==================================================>      (8 + 1) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline concluído para v2!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "run_pipeline(\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6bfce40a-caf9-4ec6-a95c-c9714ffb87f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ingestão dos dados de v3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 22:58:17 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Silver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 22:58:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 22:58:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 22:58:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Gold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 22:58:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 22:58:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 22:59:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 871:==================================================>      (8 + 1) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline concluído para v3!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "run_pipeline(\"v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb117e",
   "metadata": {},
   "source": [
    "Nessa etapa, o principal desafio no uso do formato Parquet para manipulação dos dados foi a tentativa de reduzir ao máximo o overhead de escrever todos os arquivos novamente. Isso acarreta em um tempo de execução e consumo de recursos maiores. Com isso, por não suportar operações de update, a alternativa encontrada foi realizar um **unionByName** entre os dados antigos e os novos removendo duplicações. Com isso esperamos que dados que não foram modificados não precisem ser processados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75cce42-24ad-448e-9753-3fecbf9182c3",
   "metadata": {},
   "source": [
    "**Task 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "896eec07-faa4-432c-8161-ddb625032550",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = \"datalake/bronze/\"\n",
    "silver_path = \"datalake/silver_delta/\"\n",
    "gold_path = \"datalake/gold_delta/\"\n",
    "\n",
    "def delta_table_exists(path):\n",
    "    return os.path.exists(path) and bool(os.listdir(path))\n",
    "\n",
    "def initialize_delta_table(df, path):\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "\n",
    "def get_song_info(tracks_v):\n",
    "    return tracks_v.select(\n",
    "        F.col(\"track_name\").alias(\"name\"),\n",
    "        F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "        F.col(\"duration_ms\").alias(\"duration_ms\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "\n",
    "def get_album_info(tracks_v):\n",
    "    return tracks_v.select(\n",
    "        F.col(\"album_name\").alias(\"name\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "\n",
    "def get_artist_info(tracks_v):\n",
    "    return tracks_v.select(\n",
    "        F.col(\"artist_name\").alias(\"name\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "\n",
    "def get_playlist_info(playlists_v):\n",
    "    return playlists_v.select(\n",
    "        F.col(\"name\").alias(\"name\"),\n",
    "        F.col(\"pid\").alias(\"pid\"),\n",
    "        F.col(\"description\").alias(\"description\"),\n",
    "        F.col(\"collaborative\").alias(\"is_collaborative\")\n",
    "    ).distinct()\n",
    "\n",
    "def get_playlist_track_info(tracks_v):\n",
    "    return tracks_v.select(\n",
    "        F.col(\"pid\").alias(\"playlist_id\"),\n",
    "        F.col(\"pos\").alias(\"position\"),\n",
    "        F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\")\n",
    "    )\n",
    "\n",
    "def update_gym_workout(playlist_info):\n",
    "    playlist_info = playlist_info.withColumn(\n",
    "        \"name\", F.when(F.col(\"pid\") == \"11992\", \"GYM WORKOUT\").otherwise(F.col(\"name\"))\n",
    "    ).withColumn(\n",
    "        \"is_collaborative\", F.when(F.col(\"pid\") == \"11992\", F.lit(True).cast(\"string\")).otherwise(F.col(\"is_collaborative\"))\n",
    "    )\n",
    "\n",
    "def update_gym_workout_with_delta(playlist_info_delta):\n",
    "    playlist_info_delta.update(\n",
    "        condition=F.col(\"pid\") == 11992,\n",
    "        set={\"name\": F.lit(\"GYM WORKOUT\"), \"is_collaborative\": F.lit(True)}\n",
    "    )\n",
    "    \n",
    "def ingest_new_data(version):\n",
    "    playlists = spark.read.json(f\"/shared/sampled/playlists_{version}.json\")\n",
    "    tracks = spark.read.json(f\"/shared/sampled/tracks_{version}.json\")\n",
    "\n",
    "    playlists.write.format(\"delta\").mode(\"append\").save(bronze_path + \"playlists/\")\n",
    "    tracks.write.format(\"delta\").mode(\"append\").save(bronze_path + \"tracks/\")\n",
    "\n",
    "    return playlists, tracks\n",
    "\n",
    "def update_silver_layer(playlists_v, tracks_v):\n",
    "\n",
    "    new_song_info = get_song_info(tracks_v)\n",
    "    new_album_info = get_album_info(tracks_v)\n",
    "    new_artist_info = get_artist_info(tracks_v)\n",
    "    new_playlist_info = get_playlist_info(playlists_v)\n",
    "    new_playlist_tracks = get_playlist_track_info(tracks_v)\n",
    "\n",
    "    if not delta_table_exists(silver_path):\n",
    "        update_gym_workout(new_playlist_info)\n",
    "\n",
    "        initialize_delta_table(new_song_info, silver_path + \"song_info\")\n",
    "        initialize_delta_table(new_album_info, silver_path + \"album_info\")\n",
    "        initialize_delta_table(new_artist_info, silver_path + \"artist_info\")\n",
    "        initialize_delta_table(new_playlist_info, silver_path + \"playlist_info\")\n",
    "        initialize_delta_table(new_playlist_tracks, silver_path + \"playlist_tracks\")\n",
    "        return\n",
    "\n",
    "    song_info = DeltaTable.forPath(spark, silver_path + \"song_info\")\n",
    "    album_info = DeltaTable.forPath(spark, silver_path + \"album_info\")\n",
    "    artist_info = DeltaTable.forPath(spark, silver_path + \"artist_info\")\n",
    "    playlist_info = DeltaTable.forPath(spark, silver_path + \"playlist_info\")\n",
    "    playlist_tracks = DeltaTable.forPath(spark, silver_path + \"playlist_tracks\")\n",
    "\n",
    "    def merge_delta(table, new_data, join_key):\n",
    "        table.alias(\"old\").merge(\n",
    "            new_data.alias(\"new\"), f\"old.{join_key} = new.{join_key}\"\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    merge_delta(song_info, new_song_info, \"track_uri\")\n",
    "    merge_delta(album_info, new_album_info, \"album_uri\")\n",
    "    merge_delta(artist_info, new_artist_info, \"artist_uri\")\n",
    "    merge_delta(playlist_info, new_playlist_info, \"pid\")\n",
    "    merge_delta(playlist_tracks, new_playlist_tracks, \"track_uri\")\n",
    "\n",
    "    update_gym_workout_with_delta(playlist_info)\n",
    "    \n",
    "def update_gold_layer():\n",
    "    playlist_info = spark.read.format(\"delta\").load(silver_path + \"playlist_info\")\n",
    "    song_info = spark.read.format(\"delta\").load(silver_path + \"song_info\")\n",
    "    album_info = spark.read.format(\"delta\").load(silver_path + \"album_info\")\n",
    "    artist_info = spark.read.format(\"delta\").load(silver_path + \"artist_info\")\n",
    "    playlist_tracks = spark.read.format(\"delta\").load(silver_path + \"playlist_tracks\")\n",
    "\n",
    "    song_info = song_info_df.withColumnRenamed(\"name\", \"song_name\")\n",
    "    album_info = album_info_df.withColumnRenamed(\"name\", \"album_name\")\n",
    "    artist_info = artist_info_df.withColumnRenamed(\"name\", \"artist_name\")\n",
    "\n",
    "    playlists_gold = playlist_info.alias(\"pi\") \\\n",
    "        .join(playlist_tracks.alias(\"pt\"), F.col(\"pi.pid\") == F.col(\"pt.playlist_id\"), \"left\") \\\n",
    "        .join(song_info.alias(\"si\"), F.col(\"pt.track_uri\") == F.col(\"si.track_uri\"), \"left\") \\\n",
    "        .groupBy(\"pi.pid\", \"pi.name\", \"pi.description\") \\\n",
    "        .agg(\n",
    "            F.count(\"pt.track_uri\").alias(\"num_tracks\"),\n",
    "            F.sum(\"si.duration_ms\").alias(\"total_duration_ms\"),\n",
    "            F.countDistinct(\"pt.artist_uri\").alias(\"num_artists\"),\n",
    "            F.countDistinct(\"pt.album_uri\").alias(\"num_albums\")\n",
    "        )\n",
    "\n",
    "    playlist_tracks_gold = playlist_tracks.join(\n",
    "            song_info.alias(\"si\"), \"track_uri\", \"inner\"\n",
    "        ).join(\n",
    "            album_info.alias(\"ali\"), \"album_uri\", \"inner\"\n",
    "        ).join(\n",
    "            artist_info.alias(\"ari\"), \"artist_uri\", \"inner\"\n",
    "        ).select(\n",
    "            \"playlist_id\",\n",
    "            \"position\",\n",
    "            \"song_name\",\n",
    "            \"album_name\",\n",
    "            \"artist_name\"\n",
    "    )\n",
    "\n",
    "    playlists_gold.write.format(\"delta\").mode(\"overwrite\").save(gold_path + \"playlist_info\")\n",
    "    playlist_tracks_gold.write.format(\"delta\").mode(\"overwrite\").save(gold_path + \"playlist_tracks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "71434631-9260-49bf-8e87-cc1f8e767031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(version_path):\n",
    "    print(f\"Iniciando ingestão dos dados de {version_path}...\")\n",
    "    playlists, tracks = ingest_new_data(version_path)\n",
    "\n",
    "    print(\"Atualizando a camada Silver...\")\n",
    "    update_silver_layer(playlists, tracks)\n",
    "\n",
    "    print(\"Atualizando a camada Gold...\")\n",
    "    update_gold_layer()\n",
    "\n",
    "    print(f\"Pipeline concluído para {version_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6cd055a9-6c68-41fc-babf-4cbe9c2c7d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ingestão dos dados de v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 23:01:29 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Silver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 23:01:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 23:01:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 23:01:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Gold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 23:01:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:01:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 23:01:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 23:02:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline concluído para v2!\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1a31fab-9b25-4706-a27a-e2b77d92bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ingestão dos dados de v3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 23:03:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Silver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 23:03:15 WARN MergeIntoCommand: Merge source has SQLMetric(id: 53286, name: Some(number of source rows), value: 126481) rows in initial scan but SQLMetric(id: 53287, name: Some(number of source rows (during repeated scan)), value: 0) rows in second scan\n",
      "25/02/07 23:03:16 WARN MergeIntoCommand: Merge source has SQLMetric(id: 53574, name: Some(number of source rows), value: 70477) rows in initial scan but SQLMetric(id: 53575, name: Some(number of source rows (during repeated scan)), value: 0) rows in second scan\n",
      "25/02/07 23:03:16 WARN MergeIntoCommand: Merge source has SQLMetric(id: 53862, name: Some(number of source rows), value: 30715) rows in initial scan but SQLMetric(id: 53863, name: Some(number of source rows (during repeated scan)), value: 0) rows in second scan\n",
      "25/02/07 23:03:17 WARN MergeIntoCommand: Merge source has SQLMetric(id: 54150, name: Some(number of source rows), value: 42014) rows in initial scan but SQLMetric(id: 54151, name: Some(number of source rows (during repeated scan)), value: 0) rows in second scan\n",
      "25/02/07 23:03:18 WARN MergeIntoCommand: Merge source has SQLMetric(id: 54438, name: Some(number of source rows), value: 414291) rows in initial scan but SQLMetric(id: 54439, name: Some(number of source rows (during repeated scan)), value: 0) rows in second scan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Gold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 23:03:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/07 23:03:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/07 23:03:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline concluído para v3!\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\"v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e81d9b9-b5be-42a9-8164-a7aef38126c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total storage used:\n",
      " - Parquet: 26.45 MB\n",
      " - Delta  : 24.61 MB\n",
      "Delta is 6.98% smaller than Parquet.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_storage_size(path):\n",
    "    total_size = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size / (1024 * 1024)  # Convert bytes to MB\n",
    "\n",
    "silver_parquet_path = \"datalake/gold_parquet/\"\n",
    "silver_delta_path = \"datalake/gold_delta/\"\n",
    "\n",
    "parquet_size = get_storage_size(silver_parquet_path)\n",
    "delta_size = get_storage_size(silver_delta_path)\n",
    "\n",
    "print(f\"Total storage used:\")\n",
    "print(f\" - Parquet: {parquet_size:.2f} MB\")\n",
    "print(f\" - Delta  : {delta_size:.2f} MB\")\n",
    "print(f\"Delta is {(parquet_size - delta_size) / parquet_size * 100:.2f}% smaller than Parquet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb78f3-680d-4c84-ad50-cef64584e26e",
   "metadata": {},
   "source": [
    "Como podemos observar, o uso do formato Delta contribuiu para melhorar o processo de atualização das camadas. Isso ocorreu com uma diminuição no tempo necessário para atualizá-las e também com uma redução no tamanho total ocupado pelos arquivos gerados em aproximadamente 7% se comparado ao processo utilizando Parquet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
