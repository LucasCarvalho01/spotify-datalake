{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e287060-d14c-4547-91f9-a80233e9ffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "SparkSession$ does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspotify-datalake\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mio.delta:delta-core_2.12:2.4.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.extensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mio.delta.sql.DeltaSparkSessionExtension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.spark_catalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.instances\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.cores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1024M\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.ui.port\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4061\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py:483\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 483\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(session\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    484\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: SparkSession$ does not exist in the JVM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"spotify-datalake\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1024M\") \\\n",
    "    .config(\"spark.ui.port\", \"4061\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c401f6c-9b46-406d-a3ff-a08f90803c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlists_v1_path = '/shared/sampled/playlists_v1.json'\n",
    "playlists_v1_df = spark.read.json(playlists_v1_path)\n",
    "tracks_v1_path = '/shared/sampled/tracks_v1.json'\n",
    "tracks_v1_df = spark.read.json(tracks_v1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07907de-b8b5-449f-8835-d58baf0a9060",
   "metadata": {},
   "source": [
    "Task 1A:\n",
    "- Silver layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c017094e-9c6e-4957-908d-88604282d9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 01:13:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "song_info_df = tracks_v1_df.select(\n",
    "    F.col(\"track_name\").alias(\"name\"),\n",
    "    F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "    F.col(\"duration_ms\").alias(\"duration_ms\"),\n",
    "    F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    ")\n",
    "\n",
    "song_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/song_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6535479d-4a44-4209-8f84-1b9e3aa9d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "album_info_df = tracks_v1_df.select(\n",
    "    F.col(\"album_name\").alias(\"name\"),\n",
    "    F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    ").distinct()\n",
    "\n",
    "album_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/album_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8edad837-3dc3-423e-a083-7820b2b2f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_info_df = tracks_v1_df.select(\n",
    "    F.col(\"artist_name\").alias(\"name\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    ").distinct()\n",
    "\n",
    "artist_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/artist_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e15c08a-eeb8-4cc3-82a0-a5d3cbcec746",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_info_df = playlists_v1_df.select(\n",
    "    F.col(\"name\").alias(\"name\"),\n",
    "    F.col(\"pid\").alias(\"pid\"),\n",
    "    F.col(\"description\").alias(\"description\"),\n",
    "    F.col(\"collaborative\").alias(\"is_collaborative\")\n",
    ")\n",
    "\n",
    "playlist_info_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/playlist_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "982bc341-efc6-4640-8880-0a7ba6425521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 01:13:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlist_tracks_df = tracks_v1_df.select(\n",
    "    F.col(\"pid\").alias(\"playlist_id\"),\n",
    "    F.col(\"pos\").alias(\"position\"),\n",
    "    F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "    F.col(\"artist_uri\").alias(\"artist_uri\"),\n",
    "    F.col(\"album_uri\").alias(\"album_uri\")\n",
    ")\n",
    "\n",
    "playlist_tracks_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/silver/playlist_tracks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe088a-1a82-4547-a2b3-9af8952a1af8",
   "metadata": {},
   "source": [
    "- Gold layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c65fae39-59cc-42f0-b99b-b617a5f60678",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_info_gold_df = tracks_v1_df.groupBy(\"pid\").agg(\n",
    "    F.count(\"track_uri\").alias(\"num_tracks\"),\n",
    "    F.sum(\"duration_ms\").alias(\"total_duration_ms\"),\n",
    "    F.countDistinct(\"artist_uri\").alias(\"num_artists\"),\n",
    "    F.countDistinct(\"album_uri\").alias(\"num_albums\")\n",
    ").join(playlist_info_df, \"pid\", \"inner\")\n",
    "\n",
    "playlist_info_gold_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/gold/playlist_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c3371fa-a23d-4a7b-8730-0662578f5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_info_df = song_info_df.withColumnRenamed(\"name\", \"song_name\")\n",
    "album_info_df = album_info_df.withColumnRenamed(\"name\", \"album_name\")\n",
    "artist_info_df = artist_info_df.withColumnRenamed(\"name\", \"artist_name\")\n",
    "\n",
    "playlist_tracks_gold_df = playlist_tracks_df.join(\n",
    "    song_info_df, \"track_uri\", \"inner\"\n",
    ").join(\n",
    "    album_info_df, \"album_uri\", \"inner\"\n",
    ").join(\n",
    "    artist_info_df, \"artist_uri\", \"inner\"\n",
    ").select(\n",
    "    \"playlist_id\",\n",
    "    \"position\",\n",
    "    \"song_name\",\n",
    "    \"album_name\",\n",
    "    \"artist_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b443932-f753-4c7a-9dbb-e400a2de8deb",
   "metadata": {},
   "source": [
    "Task 1B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adde54b5-bb87-4b13-aeac-b59de47ed58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlist_info_gold_df.write.format(\"json\").mode(\"overwrite\").save(\"datalake/gold_json/playlist_info\")\n",
    "playlist_tracks_gold_df.write.format(\"json\").mode(\"overwrite\").save(\"datalake/gold_json/playlist_tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "523c46ba-ae78-4a70-8b4c-c87db16749d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 01:14:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/04 01:14:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlist_info_gold_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/gold_parquet/playlist_info\")\n",
    "playlist_tracks_gold_df.write.format(\"parquet\").mode(\"overwrite\").save(\"datalake/gold_parquet/playlist_tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0dd4f6e-e0b0-46d8-9606-52f12646b61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de carregamento (JSON): 0.2530093193054199 segundos\n",
      "Tempo de carregamento (Parquet): 0.16831183433532715 segundos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# tempo de load em json\n",
    "start_time = time.time()\n",
    "json_df = spark.read.json(\"datalake/gold_json/playlist_info\")\n",
    "json_df.count()\n",
    "json_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Tempo de carregamento (JSON): {json_load_time} segundos\")\n",
    "\n",
    "# tempo de load em parquet\n",
    "start_time = time.time()\n",
    "parquet_df = spark.read.parquet(\"datalake/gold_parquet/playlist_info\")\n",
    "parquet_df.count()\n",
    "parquet_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Tempo de carregamento (Parquet): {parquet_load_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5005e",
   "metadata": {},
   "source": [
    "**Task 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5722d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = \"datalake/bronze/\"\n",
    "silver_path = \"datalake/silver/\"\n",
    "gold_path = \"datalake/gold_parquet/\"\n",
    "\n",
    "def ingest_new_data(version):\n",
    "    playlists = spark.read.json(f\"/shared/sampled/playlists_{version}.json\")\n",
    "    tracks = spark.read.json(f\"/shared/sampled/tracks_{version}.json\")\n",
    "\n",
    "    playlists.write.mode(\"append\").parquet(bronze_path + \"playlists/\")\n",
    "    tracks.write.mode(\"append\").parquet(bronze_path + \"tracks/\")\n",
    "\n",
    "    return playlists, tracks\n",
    "\n",
    "def update_silver_layer(playlists_v, tracks_v):\n",
    "    \n",
    "    song_info = spark.read.parquet(silver_path + \"song_info\")\n",
    "    album_info = spark.read.parquet(silver_path + \"album_info\")\n",
    "    artist_info = spark.read.parquet(silver_path + \"artist_info\")\n",
    "    playlist_info = spark.read.parquet(silver_path + \"playlist_info\")\n",
    "    playlist_tracks = spark.read.parquet(silver_path + \"playlist_tracks\")\n",
    "\n",
    "    new_song_info = tracks_v.select(\n",
    "        F.col(\"track_name\").alias(\"name\"),\n",
    "        F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "        F.col(\"duration_ms\").alias(\"duration_ms\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "    song_info = song_info.unionByName(new_song_info, allowMissingColumns=True).dropDuplicates([\"track_uri\"])\n",
    "\n",
    "    new_album_info = tracks_v.select(\n",
    "        F.col(\"album_name\").alias(\"name\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "    album_info = album_info.unionByName(new_album_info, allowMissingColumns=True).dropDuplicates([\"album_uri\"])\n",
    "\n",
    "    new_artist_info = tracks_v.select(\n",
    "        F.col(\"artist_name\").alias(\"name\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "    artist_info = artist_info.unionByName(new_artist_info, allowMissingColumns=True).dropDuplicates([\"artist_uri\"])\n",
    "\n",
    "    new_playlist_info = playlists_v.select(\n",
    "        F.col(\"name\").alias(\"name\"),\n",
    "        F.col(\"pid\").alias(\"pid\"),\n",
    "        F.col(\"description\").alias(\"description\"),\n",
    "        F.col(\"collaborative\").alias(\"is_collaborative\")\n",
    "    ).distinct()\n",
    "    playlist_info = playlist_info.unionByName(new_playlist_info, allowMissingColumns=True).dropDuplicates([\"pid\"])\n",
    "\n",
    "    new_playlist_tracks = tracks_v.select(\n",
    "        F.col(\"pid\").alias(\"playlist_id\"),\n",
    "        F.col(\"pos\").alias(\"position\"),\n",
    "        F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\")\n",
    "    )\n",
    "    playlist_tracks = playlist_tracks.unionByName(new_playlist_tracks, allowMissingColumns=True).dropDuplicates([\"playlist_id\", \"track_uri\"])\n",
    "\n",
    "    playlist_info = playlist_info.withColumn(\n",
    "        \"name\", F.when(F.col(\"pid\") == \"11992\", \"GYM WORKOUT\").otherwise(F.col(\"name\"))\n",
    "    ).withColumn(\n",
    "        \"is_collaborative\", F.when(F.col(\"pid\") == \"11992\", F.lit(True).cast(\"string\")).otherwise(F.col(\"is_collaborative\"))\n",
    "    )\n",
    "\n",
    "    song_info.write.mode(\"overwrite\").parquet(silver_path + \"song_info\")\n",
    "    album_info.write.mode(\"overwrite\").parquet(silver_path + \"album_info\")\n",
    "    artist_info.write.mode(\"overwrite\").parquet(silver_path + \"artist_info\")\n",
    "    playlist_info.write.mode(\"overwrite\").parquet(silver_path + \"playlist_info\")\n",
    "    playlist_tracks.write.mode(\"overwrite\").parquet(silver_path + \"playlist_tracks\")\n",
    "\n",
    "def update_gold_layer():\n",
    "    playlist_info = spark.read.parquet(silver_path + \"playlist_info\")\n",
    "    song_info = spark.read.parquet(silver_path + \"song_info\")\n",
    "    playlist_tracks = spark.read.parquet(silver_path + \"playlist_tracks\")\n",
    "\n",
    "    playlists_gold = playlist_info.alias(\"pi\") \\\n",
    "        .join(playlist_tracks.alias(\"pt\"), F.col(\"pi.pid\") == F.col(\"pt.playlist_id\"), \"left\") \\\n",
    "        .join(song_info.alias(\"si\"), F.col(\"pt.track_uri\") == F.col(\"si.track_uri\"), \"left\") \\\n",
    "        .groupBy(\"pi.pid\", \"pi.name\", \"pi.description\") \\\n",
    "        .agg(\n",
    "            F.count(\"pt.track_uri\").alias(\"num_tracks\"),\n",
    "            F.sum(\"si.duration_ms\").alias(\"total_duration_ms\"),\n",
    "            F.countDistinct(\"pt.artist_uri\").alias(\"num_artists\"),\n",
    "            F.countDistinct(\"pt.album_uri\").alias(\"num_albums\")\n",
    "        )\n",
    "\n",
    "    playlist_tracks_gold = playlist_tracks.join(\n",
    "        song_info_df, \"track_uri\", \"inner\"\n",
    "    ).join(\n",
    "        album_info_df, \"album_uri\", \"inner\"\n",
    "    ).join(\n",
    "        artist_info_df, \"artist_uri\", \"inner\"\n",
    "    ).select(\n",
    "        \"playlist_id\",\n",
    "        \"position\",\n",
    "        \"song_name\",\n",
    "        \"album_name\",\n",
    "        \"artist_name\"\n",
    "    )\n",
    "\n",
    "    playlists_gold.write.mode(\"overwrite\").parquet(gold_path + \"playlist_info\")\n",
    "    playlist_tracks_gold.write.mode(\"overwrite\").parquet(gold_path + \"playlist_tracks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c31423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(version_path):\n",
    "    print(f\"Iniciando ingestão dos dados de {version_path}...\")\n",
    "    playlists, tracks = ingest_new_data(version_path)\n",
    "\n",
    "    print(\"Atualizando a camada Silver...\")\n",
    "    update_silver_layer(playlists, tracks)\n",
    "\n",
    "    print(\"Atualizando a camada Gold...\")\n",
    "    update_gold_layer()\n",
    "\n",
    "    print(f\"Pipeline concluído para {version_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13fd205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ingestão dos dados de v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 01:38:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Silver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 01:38:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/04 01:38:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/04 01:38:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Gold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 01:38:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:38:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:39:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/04 01:39:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 315:==================================================>      (8 + 1) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline concluído para v2!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "run_pipeline(\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bfce40a-caf9-4ec6-a95c-c9714ffb87f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ingestão dos dados de v3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 01:39:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Silver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 01:39:52 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/04 01:39:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/04 01:39:57 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a camada Gold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/04 01:40:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/04 01:40:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 387:==================================================>      (8 + 1) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline concluído para v3!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "run_pipeline(\"v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75cce42-24ad-448e-9753-3fecbf9182c3",
   "metadata": {},
   "source": [
    "**Task 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896eec07-faa4-432c-8161-ddb625032550",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = \"datalake/bronze/\"\n",
    "silver_path = \"datalake/silver_delta/\"\n",
    "gold_path = \"datalake/gold_delta/\"\n",
    "\n",
    "def ingest_new_data(version):\n",
    "    playlists = spark.read.json(f\"/shared/sampled/playlists_{version}.json\")\n",
    "    tracks = spark.read.json(f\"/shared/sampled/tracks_{version}.json\")\n",
    "\n",
    "    playlists.write.format(\"delta\").mode(\"append\").save(bronze_path + \"playlists/\")\n",
    "    tracks.write.format(\"delta\").mode(\"append\").save(bronze_path + \"tracks/\")\n",
    "\n",
    "    return playlists, tracks\n",
    "\n",
    "def update_silver_layer(playlists_v, tracks_v):\n",
    "\n",
    "    song_info = spark.read.parquet(silver_path + \"song_info\")\n",
    "    album_info = spark.read.parquet(silver_path + \"album_info\")\n",
    "    artist_info = spark.read.parquet(silver_path + \"artist_info\")\n",
    "    playlist_info = spark.read.parquet(silver_path + \"playlist_info\")\n",
    "    playlist_tracks = spark.read.parquet(silver_path + \"playlist_tracks\")\n",
    "\n",
    "    new_song_info = tracks_v.select(\n",
    "        F.col(\"track_name\").alias(\"name\"),\n",
    "        F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "        F.col(\"duration_ms\").alias(\"duration_ms\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "    song_info = song_info.unionByName(new_song_info, allowMissingColumns=True).dropDuplicates([\"track_uri\"])\n",
    "\n",
    "    new_album_info = tracks_v.select(\n",
    "        F.col(\"album_name\").alias(\"name\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "    album_info = album_info.unionByName(new_album_info, allowMissingColumns=True).dropDuplicates([\"album_uri\"])\n",
    "\n",
    "    new_artist_info = tracks_v.select(\n",
    "        F.col(\"artist_name\").alias(\"name\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\")\n",
    "    ).distinct()\n",
    "    artist_info = artist_info.unionByName(new_artist_info, allowMissingColumns=True).dropDuplicates([\"artist_uri\"])\n",
    "\n",
    "    new_playlist_info = playlists_v.select(\n",
    "        F.col(\"name\").alias(\"name\"),\n",
    "        F.col(\"pid\").alias(\"pid\"),\n",
    "        F.col(\"description\").alias(\"description\"),\n",
    "        F.col(\"collaborative\").alias(\"is_collaborative\")\n",
    "    ).distinct()\n",
    "    playlist_info = playlist_info.unionByName(new_playlist_info, allowMissingColumns=True).dropDuplicates([\"pid\"])\n",
    "\n",
    "    new_playlist_tracks = tracks_v.select(\n",
    "        F.col(\"pid\").alias(\"playlist_id\"),\n",
    "        F.col(\"pos\").alias(\"position\"),\n",
    "        F.col(\"track_uri\").alias(\"track_uri\"),\n",
    "        F.col(\"artist_uri\").alias(\"artist_uri\"),\n",
    "        F.col(\"album_uri\").alias(\"album_uri\")\n",
    "    )\n",
    "    playlist_tracks = playlist_tracks.unionByName(new_playlist_tracks, allowMissingColumns=True).dropDuplicates([\"playlist_id\", \"track_uri\"])\n",
    "\n",
    "    def merge_delta(table, new_data, join_key):\n",
    "        table.alias(\"old\").merge(\n",
    "            new_data.alias(\"new\"), f\"old.{join_key} = new.{join_key}\"\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    merge_delta(song_info, new_song_info, \"track_uri\")\n",
    "    merge_delta(album_info, new_album_info, \"album_uri\")\n",
    "    merge_delta(artist_info, new_artist_info, \"artist_uri\")\n",
    "    merge_delta(playlist_info, new_playlist_info, \"pid\")\n",
    "    merge_delta(playlist_tracks, new_playlist_tracks, \"track_uri\")\n",
    "    \n",
    "    playlist_info.update(\n",
    "        condition=F.col(\"pid\") == 11992,\n",
    "        set={\"name\": F.lit(\"GYM WORKOUT\"), \"collaborative\": F.lit(True)}\n",
    "    )\n",
    "\n",
    "def update_gold_layer():\n",
    "    playlist_info = spark.read.format(\"delta\").load(silver_path + \"playlist_info\")\n",
    "    song_info = spark.read.format(\"delta\").load(silver_path + \"song_info\")\n",
    "    playlist_tracks = spark.read.format(\"delta\").load(silver_path + \"playlist_tracks\")\n",
    "\n",
    "    playlists_gold = playlist_info.alias(\"pi\") \\\n",
    "        .join(playlist_tracks.alias(\"pt\"), F.col(\"pi.pid\") == F.col(\"pt.playlist_id\"), \"left\") \\\n",
    "        .join(song_info.alias(\"si\"), F.col(\"pt.track_uri\") == F.col(\"si.track_uri\"), \"left\") \\\n",
    "        .groupBy(\"pi.pid\", \"pi.name\", \"pi.description\") \\\n",
    "        .agg(\n",
    "            F.count(\"pt.track_uri\").alias(\"num_tracks\"),\n",
    "            F.sum(\"si.duration_ms\").alias(\"total_duration_ms\"),\n",
    "            F.countDistinct(\"pt.artist_uri\").alias(\"num_artists\"),\n",
    "            F.countDistinct(\"pt.album_uri\").alias(\"num_albums\")\n",
    "        )\n",
    "\n",
    "    playlist_tracks_gold = playlist_tracks.join(\n",
    "        song_info_df, \"track_uri\", \"inner\"\n",
    "    ).join(\n",
    "        album_info_df, \"album_uri\", \"inner\"\n",
    "    ).join(\n",
    "        artist_info_df, \"artist_uri\", \"inner\"\n",
    "    ).select(\n",
    "        \"playlist_id\",\n",
    "        \"position\",\n",
    "        \"song_name\",\n",
    "        \"album_name\",\n",
    "        \"artist_name\"\n",
    "    )\n",
    "\n",
    "    playlists_gold.write.format(\"delta\").mode(\"overwrite\").save(gold_path + \"playlist_info\")\n",
    "    playlist_tracks_gold.write.format(\"delta\").mode(\"overwrite\").save(gold_path + \"playlist_tracks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71434631-9260-49bf-8e87-cc1f8e767031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(version_path):\n",
    "    print(f\"Iniciando ingestão dos dados de {version_path}...\")\n",
    "    playlists, tracks = ingest_new_data(version_path)\n",
    "\n",
    "    print(\"Atualizando a camada Silver...\")\n",
    "    update_silver_layer(playlists, tracks)\n",
    "\n",
    "    print(\"Atualizando a camada Gold...\")\n",
    "    update_gold_layer()\n",
    "\n",
    "    print(f\"Pipeline concluído para {version_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd055a9-6c68-41fc-babf-4cbe9c2c7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pipeline(\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a31fab-9b25-4706-a27a-e2b77d92bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pipeline(\"v3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
